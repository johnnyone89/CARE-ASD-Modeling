{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e633e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning models and tools\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, matthews_corrcoef, cohen_kappa_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (assuming the dataset is already loaded and categorical variables are encoded)\n",
    "# For demonstration, let's assume the dataset is loaded into 'df'\n",
    "df = pd.read_csv('ASD_Traits_Study_Data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "X = df.drop(columns=['ASD_traits'])\n",
    "y = df['ASD_traits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Verify the shapes of training and testing datasets\n",
    "print(f\"Training set: {X_train.shape}, Testing set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (if necessary)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter ranges for RandomizedSearchCV as per the paper (Table V)\n",
    "\n",
    "# Import scipy for distributions\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Randomized Search parameter distributions\n",
    "random_grid = {\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [10, 120, 230, 340, 450, 560, 670, 780, 890, 1000],\n",
    "    'min_samples_split': [2, 5, 10, 14],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328113e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter values for GridSearchCV as per the paper (Table V)\n",
    "grid_param = {\n",
    "    'n_estimators': [600, 700, 800, 900, 1000],\n",
    "    'max_features': ['sqrt'],\n",
    "    'max_depth': [560],\n",
    "    'min_samples_split': [3, 4, 5, 6, 7],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'criterion': ['entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477320a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=100,  # Number of parameter settings that are sampled\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_random_model = random_search.best_estimator_\n",
    "best_random_params = random_search.best_params_\n",
    "\n",
    "print(\"Best Parameters from Randomized Search:\")\n",
    "print(best_random_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "print(\"Evaluating the best model from RandomizedSearchCV on the test set...\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred_random = best_random_model.predict(X_test_scaled)\n",
    "y_test_prob_random = best_random_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy_random = accuracy_score(y_test, y_test_pred_random)\n",
    "precision_random = precision_score(y_test, y_test_pred_random)\n",
    "recall_random = recall_score(y_test, y_test_pred_random)\n",
    "f1_random = f1_score(y_test, y_test_pred_random)\n",
    "roc_auc_random = roc_auc_score(y_test, y_test_prob_random)\n",
    "mcc_random = matthews_corrcoef(y_test, y_test_pred_random)\n",
    "kappa_random = cohen_kappa_score(y_test, y_test_pred_random)\n",
    "\n",
    "# Print test set metrics\n",
    "print(\"Test Set Metrics (Randomized Search):\")\n",
    "print(f\"Accuracy: {accuracy_random:.4f}, Precision: {precision_random:.4f}, Recall: {recall_random:.4f}\")\n",
    "print(f\"F1-Score: {f1_random:.4f}, ROC-AUC: {roc_auc_random:.4f}\")\n",
    "print(f\"MCC: {mcc_random:.4f}, Cohen's Kappa: {kappa_random:.4f}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76852d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=grid_param,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_grid_model = grid_search.best_estimator_\n",
    "best_grid_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best Parameters from Grid Search:\")\n",
    "print(best_grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b40bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "print(\"Evaluating the best model from GridSearchCV on the test set...\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred_grid = best_grid_model.predict(X_test_scaled)\n",
    "y_test_prob_grid = best_grid_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy_grid = accuracy_score(y_test, y_test_pred_grid)\n",
    "precision_grid = precision_score(y_test, y_test_pred_grid)\n",
    "recall_grid = recall_score(y_test, y_test_pred_grid)\n",
    "f1_grid = f1_score(y_test, y_test_pred_grid)\n",
    "roc_auc_grid = roc_auc_score(y_test, y_test_prob_grid)\n",
    "mcc_grid = matthews_corrcoef(y_test, y_test_pred_grid)\n",
    "kappa_grid = cohen_kappa_score(y_test, y_test_pred_grid)\n",
    "\n",
    "# Print test set metrics\n",
    "print(\"Test Set Metrics (Grid Search):\")\n",
    "print(f\"Accuracy: {accuracy_grid:.4f}, Precision: {precision_grid:.4f}, Recall: {recall_grid:.4f}\")\n",
    "print(f\"F1-Score: {f1_grid:.4f}, ROC-AUC: {roc_auc_grid:.4f}\")\n",
    "print(f\"MCC: {mcc_grid:.4f}, Cohen's Kappa: {kappa_grid:.4f}\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa0d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to compare the performance\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'MCC', \"Cohen's Kappa\"],\n",
    "    'Randomized Search': [accuracy_random, precision_random, recall_random, f1_random, roc_auc_random, mcc_random, kappa_random],\n",
    "    'Grid Search': [accuracy_grid, precision_grid, recall_grid, f1_grid, roc_auc_grid, mcc_grid, kappa_grid]\n",
    "})\n",
    "\n",
    "print(\"Comparison of Model Performance:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e419132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze if RandomizedSearchCV and GridSearchCV yielded similar performance\n",
    "print(\"Performance comparison between RandomizedSearchCV and GridSearchCV:\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
